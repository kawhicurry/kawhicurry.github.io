<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>分类: AI | Kawhicurry's Blog</title><meta name="author" content="kawhicurry"><meta name="copyright" content="kawhicurry"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLM出来之后，一直很想要一个自己的知识库系统。很早之前就选好了Obsidian当自己的知识库系统，把自己的各种知识收集进去。Ob的一大特点就是所有东西都是直接装在目录里的，顺便可以当成一个目录管理器用。这样做的意义在于，以后可以自己控制LLM处理这些文件的细节。 其实之前玩过本地的Stable-D"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/profile/favicon.jpg"><link rel="canonical" href="https://kawhicurry.github.io/AI/47184172/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-UXVbU6tNCR"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-70WJTE71NM"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-70WJTE71NM');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":1000},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: kawhicurry","link":"链接: ","source":"来源: Kawhicurry's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#d8dee9","bgDark":"#2e3440","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '分类: AI',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-22 19:54:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kawhicurry's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/profile/avatar.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/img_1327.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Kawhicurry's Blog"><span class="site-name">Kawhicurry's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">第一次LLM学习之旅</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-22T11:54:09.896Z" title="发表于 2025-02-22 19:54:09">2025-02-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-22T11:54:09.896Z" title="更新于 2025-02-22 19:54:09">2025-02-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="第一次LLM学习之旅"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>LLM出来之后，一直很想要一个自己的知识库系统。很早之前就选好了Obsidian当自己的知识库系统，把自己的各种知识收集进去。<br>Ob的一大特点就是所有东西都是直接装在目录里的，顺便可以当成一个目录管理器用。<br>这样做的意义在于，以后可以自己控制LLM处理这些文件的细节。</p>
<p>其实之前玩过本地的<code>Stable-Diffustion</code>，效果非常好，已经是一个成熟的调包侠了，会自己从<code>HuggingFace</code>上找model，然后找提示词网站来组装生成图。<br>但是对于语言大模型，部署的过程一直不太顺利。最开始我的选择是国产的<code>ChatGlm-6B</code>。<br>但是在过程中多次出现各种跑不起来的情况，最后恍然大悟Windows不好使，还是得上Linux。<br>当时其实有Hype-V的Linux虚拟机，用来写我的eBPF毕设。<br>后面发现Hype-V里没法轻松把显卡挂进去，又对WSL有一种莫名的抗拒，于是索性没有往下搞。</p>
<p>最近突然心血来潮，又想来试试大模型有什么别的学习路径，这下发现了<code>Ollama</code>这个好东西。</p>
<h1 id="开端：Ollama，想做大模型界的Docker"><a href="#开端：Ollama，想做大模型界的Docker" class="headerlink" title="开端：Ollama，想做大模型界的Docker"></a>开端：Ollama，想做大模型界的Docker</h1><p>如题所示，Ollama做了一些相当不错的封装。我把我心目中最终要的命令列在下面先，不妨来看看：</p>
<pre class="line-numbers language-none"><code class="language-none">ollama pull qwen2
ollama list
ollama run qwen2
ollama ps<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>怎么样？如果你也是个Linux老鸟的话你会发现，<code>ollama</code>这个命令和<code>docker</code>似乎一模一样。<br>这也是ollama的追求，它似乎想做LLM时代的Docker。</p>
<p>当然，上面这四行命令中，最吸引人的当然是第一条和第三条。<br>我依然记得下载ChatGlm的时候，要去清华的网盘里把所有bin文件都点一遍下载。<br>虽然它给了压缩按钮，但是由于目录太大，它不让我压缩。<br>而ollama只需要一个pull，一个也许是你熟知的大模型就拉取到你本地了。<br>这个拉取过程支持断点续传，并且拉取的似乎也是一种分层模型，颇有docker中union filesystem的味道。</p>
<p>而ollama run更是惊艳，相比于其他llm那配置了半天还要自己写代码配置的方式，一行ollma run之后，一个对话框就出现了。<br>一个最简单的对话过程就开始，这和我们在网页上使用的各种什么大模型，没有任何区别。</p>
<p>当这个存在于终端里的对话框出现的时候，我突然意识到，这就是<strong>LLM本来的样子</strong>。</p>
<h1 id="悟道：Transformer，也许仍然是个函数"><a href="#悟道：Transformer，也许仍然是个函数" class="headerlink" title="悟道：Transformer，也许仍然是个函数"></a>悟道：Transformer，也许仍然是个函数</h1><p>在真正接触本地LLM之前，我一直有一个疑问，LLM到底是不是真正的通用人工智能？<br>但当我把本地LLM跑起来之后，我意识到这玩意远比我想象的要“结构简单”。</p>
<p>目前大多数LLM使用的是一种叫Transformer的架构。其实除了它还有一些别的架构，比如LSTM。<br>虽然我也不知道这些架构具体有哪些差异，但从结论上来看，Transformer解决了一个非常重要的问题：当神经网络层数增加时，对算力的需求不会陡增。<br>而这意味着，只要使用大量算力进行计算，效果就一定会变好。这在一定程度上解释了，为什么GPT-2到GPT-3有着如此大的提升——架构对了，怼算力就完事了。</p>
<p>至于Transformer究竟是什么？我只是个小小的工程师，这种专业问题还是问问搞AI的大佬吧。对于一个工程师来说，Transformer就是一个函数。我说的函数不是数学上那个函数，数学上的函数很好理解，毕竟啥都能当成函数。我说的是我们编程的那个函数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">transformer</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> sth<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>怎么样，是不是眼熟了许多？为什么要用这种描述呢？<br>这种认知意味着，<em>当相关软件架构成熟后，任何工程师都可以将LLM作为自身软件的一部分</em>。<br>但同时，由于LLM本身即是一个Transformer（这里我们暂时不考虑其他架构），我们只需单独处理它的输入和输出即可。</p>
<p>那么下一个问题是，这是一个怎样的函数？<br>在讨论这个问题前，让我们先来看看最常见的GPT模型，有没有想过GPT的全名是什么？<br>GPT全称Generative Pre-trained Transformer。从这个名字上我们可以看出来两件事。<br>首先，第一个问题在于这个Generative，与Artificial general intelligence（通用人工智能）不同，GPT很“谦虚”，但也很精确地描述了自己是什么，它不是AGI，而是单纯一个预训练过的大号Transformer。<br>这与我们上面得出来的结论是一致的。<br>其次，第二个问题在于这个pre-trained（预训练）。<br>这个过程本身其实很好理解，就是爬取网络上的各种知识给它，进行训练。<br>但这种训练意味着，这个函数的内部被改变了。<br>而函数内部的改变意味着训练前后，对于同一输入，给出的输出可能会大大不同，因为此时函数内部发生了变化。<br>现在让我们重新来审视这个“函数”。它应该长这个样：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
<span class="token keyword">def</span> <span class="token function">pre_train</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> parser

<span class="token keyword">def</span> <span class="token function">transformer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> parser<span class="token punctuation">(</span>sth<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>同时，这种预训练过程还导出了另一个结论——你有没有担心过AI替代人类这种问题？<br>现在你应该知道，这玩意根本做不到这种事，它就是个简单的函数，接受参数（我们一般管这个叫prompt，提示词），给出输出。更何况，它甚至不会上网自个找资料——因为它是预训练的，而不是实时的。</p>
<p>现在想必你会有一个疑问，能不能让他上网？网上的知识浩如烟海，要是能上网岂不是无敌了？</p>
<h1 id="增强：RAG，给LLM插上翅膀"><a href="#增强：RAG，给LLM插上翅膀" class="headerlink" title="增强：RAG，给LLM插上翅膀"></a>增强：RAG，给LLM插上翅膀</h1><p>既然Transformer可以处理我们的问题，也可以处理网上的知识，那我们把网上的知识和我们的问题一起交给它不就好了。只不过，在获取网上的知识这一步，我们需要多一些步骤，把网上的知识处理成文本形式。<br>或者更近一步呢？也许不只是网上的知识，也许可以是本地的。也不是不只是文字的，也可以是图片，可以是文档甚至是更多可能的格式。</p>
<p>上面描述的过程，在LLM这里叫做RAG（Retrieval Augmented Generation，检索增强生成）。<br>简单来说，就是在使用大模型回答问题前，先帮大模型搜罗一部分资料，这种资料往往不被包含在大模型预训练时获取的部分，且这种资料一般有较强的实时性。</p>
<p>RAG应用一般有以下几个步骤：</p>
<ol>
<li>加载数据，从上面提到的各类数据里加载进来</li>
<li>分词，由于数据众多，所以要依据一定规则将其分片</li>
<li>存储，分词后就可以建立索引，方便后续的提取</li>
<li>取值，从索引中找到数据，进行提取</li>
<li>生成，依据提取出来的数据<strong>和</strong>用户的问题一同输出给大模型，生成结果</li>
</ol>
<p>以代码形式展示就是：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
question <span class="token operator">=</span> <span class="token string">"?"</span>

<span class="token keyword">def</span> <span class="token function">RAG</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  raw_documents <span class="token operator">=</span> load<span class="token punctuation">(</span><span class="token punctuation">)</span>
  chunks <span class="token operator">=</span> split<span class="token punctuation">(</span>raw_documents<span class="token punctuation">)</span>
  index <span class="token operator">=</span> store<span class="token punctuation">(</span>chunks<span class="token punctuation">)</span>
  prompt <span class="token operator">=</span> retrieve<span class="token punctuation">(</span>index<span class="token punctuation">)</span>
  result <span class="token operator">=</span> generate<span class="token punctuation">(</span>prompt <span class="token operator">+</span> question<span class="token punctuation">)</span>
  <span class="token keyword">return</span> result
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>有了这种能力，相当于给大模型插上了翅膀，大模型的能力不仅仅局限于预训练时的数据，也对实时的数据有了处理能力。</p>
<h1 id="扮演：Prompt，戏精上身，但更懂你"><a href="#扮演：Prompt，戏精上身，但更懂你" class="headerlink" title="扮演：Prompt，戏精上身，但更懂你"></a>扮演：Prompt，戏精上身，但更懂你</h1><p>现在，是时候让LLM来帮忙干点活了，比如算算当下很火的数学题：3.9和3.11谁大？<br>为了防止一些非计算机领域或者不了解计算机版本号的同学get不到这个问题的精髓之处，还是稍微做些补充。从数学上来说，当然是3.90大于3.11，但在软件工程里，这种数字通常表示大版本+小版本，也就是3.09小于3.11。<br>当大家去问这个问题的时候，可能由于训练的预料不同得到不同的结果。<br>比如使用计算机知识语料进行训练的话，可能就是3.9&lt;3.11。<br>但如果训练时专门对数学类问题进行过调优的话，就能很好地解释3.9&gt;3.11。<br>倘若一个没有计算机知识的用户碰到了前一种大模型，就容易让人有种感觉：大模型好蠢啊。</p>
<p>但其实对人来来说，这个问题也未必是那么好回答的。<br>倘若网上冷不丁地弹出一个窗口问我这俩哪个大，我还真得楞一下，然后不知所措——因为我能意识到，这道题目似乎是少了条件的，他可没说是软件版本号还是数字比大小。<br>但如果是人在问我，那我的判断也许多一丝把握。<br>如果是我的工程师同学来问我，那十有八九是版本号了，如果是我的弟弟妹妹来问我，十有八九是写作业偷懒不想自己算了，如果是我的兄弟——那估计是看到了这个帖子，过来消遣洒家的。</p>
<p>看，为什么我可以分辨出这些？当然是我了解这些人，知道他们的特点，从而可以给他们回答。那为啥LLM不行呢？<br>回到那个网上冷不丁弹窗口的场景，LLM看到的就是一个这样的场景。<br>它既不知道你是谁，也不知道你在搞什么鸡毛，但是LLM的责任心（其实也没啥责任心，都是预训练留下的结果）让它必须回答。那能怎么办呢？<br>想想你当年上课的时候被突然抽起来回答问题的场景就好了——根据以前的经验，猜一个嘛，你大喊一声选C，老师说这是填空题。<br>这就是上面这个场景中LLM无法回答的原因，它并非真的不知道怎么回答，也不是随便乱猜。<br>只是你啥也没告诉它，所以它只能从过往的经验里，挑了个概率最大的告诉你，简称挑最大的答。</p>
<p>那我们可以做什么呢？简单!告诉它上下文就好了。如果你要问数学问题，你就告诉LLM：</p>
<blockquote>
<p>数学上3.9和3.11谁更大？</p>
</blockquote>
<p>如果你要问版本号，那你就说：</p>
<blockquote>
<p>计算机版本号上3.9和3.11谁更大</p>
</blockquote>
<p>以上，便是提示词（prompt）工程，通过提示词来诱导LLM给出我们期望的答案。<br>无论未来的LLM发展到什么地步，这种提示词工程都将存在下去，<br>因为<strong>人类生存的世界里，上下文永远是连续的</strong>。<br>但每次LLM启动时，它都会回到它诞生（预训练完成）的那一刻。</p>
<p>当我们追求通用能力的时候，精确性就会下降，反之亦然。<br>而GPT的开创意义就在这里，它选择了用最广泛，最Generative的预料来训练，然后在推理阶段根据大量的提示词来进行上下文的推理计算，得到的也是最接近人类逻辑的答案。</p>
<p>最好的提示方式是什么？想比与念一些莫名其妙地咒语，目前最佳的方式似乎是<code>扮演</code>。<br>扮成一个老师、一个工程师或者猫娘（？）<br>感觉提示词展开又是一个大工程，网上已经有相当多的教程了，还是请读着自行搜索吧。</p>
<h1 id="应用：Agent，人机交互的最优解"><a href="#应用：Agent，人机交互的最优解" class="headerlink" title="应用：Agent，人机交互的最优解"></a>应用：Agent，人机交互的最优解</h1><p>等等，总不能到最后AI都去写诗歌了，我们还在打工吧。<br>我们这么努力发展AI可不是为了养个爹，所以怎么让AI来帮我们干活呢？<br>那得先想想我们在干啥？写代码、画图表，一万个人心中也许有一万个想法。</p>
<p>但是通过上面的介绍，你应该明白——LLM其实什么也做不了，因为LLM就是LLM。它接受一个输入，给出一个输出，它不是人，甚至不是机器人。它不会帮你做出任何事。它唯一会的，就是根据你的输入，结合各种其他的给定，给出一个输出。这也是为什么在上面的很多地方，我会用函数来描述LLM，它只是处理数据，并不做任何实际操作。</p>
<p>不过话说回来，既然能是个函数，那也就意味着，它可以被“组装”进现有的软件体系中，作为软件的一部分。比如我们可以给出几个选项，然后迫使LLM从这几个选项中选出一个可能的，然后交给软件来执行。</p>
<p>当然，实际上我们要考虑的要更多，比如LLM如何取数据，取错了怎么办，或者是没照着选项怎么办，所以很多情况下，我们需要引入更多代码来解决这个问题。上面这种开发方式，就是当下火热的Agent模式。</p>
<p>以我个人的观点，这种开发模式中最重要的一点就是：给LLM配备工具。LLM本身是不会使用工具的，这和很多大家看到的能自适应一些环境的机器人不同。只有给LLM提供工具，并让LLM来选择使用什么工具、以何种方式使用工具，才有望让LLM真正为人类服务。现在我们再看LLM，你会发现，对话，也许只是它最最最简单的那个应用，Agent才是LLM的未来形态。</p>
<h1 id="未来：LLM，钉子何在？"><a href="#未来：LLM，钉子何在？" class="headerlink" title="未来：LLM，钉子何在？"></a>未来：LLM，钉子何在？</h1><p>关于LLM我一直有一些暴论，其中一个也许大多数人和我持有一样的观点：LLM的输出是不可信的。</p>
<p>这是一件相当重要的事情，LLM和我们过去见过的计算机或其他较为稳定的控制系统不同，它不是稳定的。我这里对于稳定的定义要相对简单一些：在任何情况下，当输入一致的时候，输出也应该一致。但很显然，就目前的观察而言，有相当多的案例下，LLM的输出是不稳定的。</p>
<p>这意味着，你无法让它像其他人类设计出来的自动控制系统一样，在一些工业场景下工作，更何况LLM的内部还是个黑盒。这一点相当程度上制约了它的应用，这是一个概率问题，如果人类设计的系统可用性是99.99%，那LLM也许是个未知的概率。</p>
<p>我尝试搜索了下，LLM的输出为何做不到稳定，是否是因为加入了某些seed或是其他的随机数机制呢？很遗憾，我似乎找不到我能看懂的答案，如果哪天有大佬看到了这个问题，我很愿意接受大佬的指导。但我确信的是，如果LLM的输出做不到绝对稳定的话，我们就没法直接把工具直接交给LLM。</p>
<p>但是反过来，我们也可以换个思路。既然LLM有如此强大的处理能力，不妨让LLM“给出建议”就好了，最终的决策，还是由人来完成好了。这也是我目前做LLM应用的思路。</p>
<p>关于未来，谁知道呢？最近看了一篇文章，其中的观点是现代的LLM其实还处于一个初级的阶段，所以需要大量的提示词工程来帮助从LLM中获得想要的答案，如果未来有一天LLM发展到了某个阶段，就不需要这么多提示词了，似乎听着像是那么回事。</p>
<p>不过有另一个好消息是，许多程序员也许不用担心失业了。LLM不会让程序员失业，相反，它需要更多优秀的程序员去建设它。未来的LLM应当是触手可及的、人人可以去建设的，而不是高高在上的、遥不可及的。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>RAG: <a target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/tutorials/rag/">https://python.langchain.com/v0.2/docs/tutorials/rag/</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://kawhicurry.github.io">kawhicurry</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kawhicurry.github.io/AI/47184172/">https://kawhicurry.github.io/AI/47184172/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kawhicurry.github.io" target="_blank">Kawhicurry's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-62037f9d1c65c8d1" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/Auto/6b018282/" title="自动化的讽刺"><img class="cover" src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/IMG_1214.JPG" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">自动化的讽刺</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/profile/avatar.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="avatar"/></div><div class="author-info__name">kawhicurry</div><div class="author-info__description">An student aiming to be an operation engineer.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kawhicurry"><i class="fab fa-github"></i><span>逛逛博主的github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kawhicurry" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:kawhicurry@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://kawhicurry.github.io/atom.xml" target="_blank" title="Rss"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我们一天天所度过的所谓日常,可能是接连不断的奇迹</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%80%E7%AB%AF%EF%BC%9AOllama%EF%BC%8C%E6%83%B3%E5%81%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%95%8C%E7%9A%84Docker"><span class="toc-number">1.</span> <span class="toc-text">开端：Ollama，想做大模型界的Docker</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%82%9F%E9%81%93%EF%BC%9ATransformer%EF%BC%8C%E4%B9%9F%E8%AE%B8%E4%BB%8D%E7%84%B6%E6%98%AF%E4%B8%AA%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">悟道：Transformer，也许仍然是个函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A2%9E%E5%BC%BA%EF%BC%9ARAG%EF%BC%8C%E7%BB%99LLM%E6%8F%92%E4%B8%8A%E7%BF%85%E8%86%80"><span class="toc-number">3.</span> <span class="toc-text">增强：RAG，给LLM插上翅膀</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%AE%E6%BC%94%EF%BC%9APrompt%EF%BC%8C%E6%88%8F%E7%B2%BE%E4%B8%8A%E8%BA%AB%EF%BC%8C%E4%BD%86%E6%9B%B4%E6%87%82%E4%BD%A0"><span class="toc-number">4.</span> <span class="toc-text">扮演：Prompt，戏精上身，但更懂你</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%EF%BC%9AAgent%EF%BC%8C%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E6%9C%80%E4%BC%98%E8%A7%A3"><span class="toc-number">5.</span> <span class="toc-text">应用：Agent，人机交互的最优解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%EF%BC%9ALLM%EF%BC%8C%E9%92%89%E5%AD%90%E4%BD%95%E5%9C%A8%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">未来：LLM，钉子何在？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">6.1.</span> <span class="toc-text">参考</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/AI/47184172/" title="第一次LLM学习之旅"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/img_1327.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第一次LLM学习之旅"/></a><div class="content"><a class="title" href="/AI/47184172/" title="第一次LLM学习之旅">第一次LLM学习之旅</a><time datetime="2025-02-22T11:54:09.896Z" title="发表于 2025-02-22 19:54:09">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Auto/6b018282/" title="自动化的讽刺"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/IMG_1214.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自动化的讽刺"/></a><div class="content"><a class="title" href="/Auto/6b018282/" title="自动化的讽刺">自动化的讽刺</a><time datetime="2025-02-22T08:41:40.000Z" title="发表于 2025-02-22 16:41:40">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Tools/undefined/" title="lock教程"><div style="background: https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/img_1327.jpg"></div></a><div class="content"><a class="title" href="/Tools/undefined/" title="lock教程">lock教程</a><time datetime="2025-02-15T06:08:14.000Z" title="发表于 2025-02-15 14:08:14">2025-02-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Daily/41551188/" title="daily-2024-07-07"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/img_1206.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="daily-2024-07-07"/></a><div class="content"><a class="title" href="/Daily/41551188/" title="daily-2024-07-07">daily-2024-07-07</a><time datetime="2024-07-06T16:34:01.000Z" title="发表于 2024-07-07 00:34:01">2024-07-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Operation/Linux/47a3710b/" title="从tc开始的一次简单的内核网络栈探索"><img src="https://cdn.jsdelivr.net/gh/kawhicurry/picgo/gallery/nord/IMG_1217.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tc开始的一次简单的内核网络栈探索"/></a><div class="content"><a class="title" href="/Operation/Linux/47a3710b/" title="从tc开始的一次简单的内核网络栈探索">从tc开始的一次简单的内核网络栈探索</a><time datetime="2023-10-29T04:30:06.000Z" title="发表于 2023-10-29 12:30:06">2023-10-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/kawhicurry/picgo/nordic/nord_alone_tree.png')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By kawhicurry</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to the world of an growing SRE.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '528affc74d9344c23daf',
      clientSecret: '41afe1cad2849c60940bd386fe0bbda659a73a16',
      repo: 'kawhicurry.github.io.comment',
      owner: 'kawhicurry',
      admin: ['kawhicurry'],
      id: '940235d787ea0f97740b9e0dbe120465',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>